# Datastore Distillation for Nearest Neighbor Machine Translation

Official Code for our paper "Datastore Distillation for Nearest Neighbor Machine Translation".

### Preparation
First, you must prepare the environment for kNN-MT using the fairseq framework. To make it concrete, you have to finish the following steps.
* You have to install the fairseq
* You have to create the overall keys and values
Please refer to the original [kNN-MT](https://github.com/urvashik/knnmt).

### Prune the Datastore
You can prune the datastore by simply running our provided shell script.

### Evaluation

